{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Library to load tabular data\n",
    "pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data (no changes)\n",
    "\n",
    "Note that here I'm pretending like 10% of the data is unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('https://raw.githubusercontent.com/mkleinbort/resource-datasets/master/ny_times_comments/CommentsApril2017_9000_interesting_columns.csv')\n",
    "\n",
    "data_train = all_data.loc[lambda x: x.index%10 != 0]\n",
    "data_test = all_data.loc[lambda x: x.index%10 == 0].drop(columns=['editorsSelection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we want to learn? (no changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_train['editorsSelection'] # Did the NY Times Editor choose to feature the comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we have available to make this prediction?\n",
    "\n",
    "Here we need to think a little, and figure out what will be available at the time when we want to make our prediction. We have many features including:\n",
    "- approveDate\n",
    "- commentBody\n",
    "- commentType\n",
    "- createDate\n",
    "- parentUserDisplayName\n",
    "- replyCoun\n",
    "- sharing\n",
    "- timespeople\n",
    "- trusted\n",
    "- updateDate\n",
    "- userDisplayNam\n",
    "- userLocation\n",
    "- inReplyTo\n",
    "- sectionName\n",
    "- newDesk\n",
    "- articleWordCount\n",
    "- printPage\n",
    "- typeOfMaterial\n",
    "\n",
    "\n",
    "But many of these features are only available AFTER the prediction would have been useful. For example, is it useful to predict whether a comment will be featured by the editor if we MUST know ahead of time how many people will reply to the comment?\n",
    "\n",
    "For right now, let's assume we only know:\n",
    "- The articleWordCount\n",
    "- The typeOfMaterial\n",
    "- The commentBody\n",
    "- The commentType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part 1: Prepare the data\n",
    "\n",
    "Somehow we need to make X into a table of only numbers. However, X is far from being numeric right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['articleWordCount','typeOfMaterial','commentType', 'commentBody']\n",
    "\n",
    "X = data_train.loc[:, FEATURES] # Select all rows, and only these columns\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make each row into only numbers. Here is one approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Takes a long time\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = embed([\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I am a sentence for which I would like to get its embedding\"])\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric = X.select_dtypes('number')\n",
    "\n",
    "X_text_values = embed(X['commentBody'].to_list())\n",
    "X_text = pd.DataFrame(data=X_text_values, columns=vectorizer.vocabulary_, index=X.index)\n",
    "\n",
    "X_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_numeric, X_text], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Model Part 1 - Continued: Prepare the test data In exactly the same way \n",
    "\n",
    "It's very easy for things to go wrong here. As a gut feeling, most of all hard to debug issues come from the test data being processed slightly differently from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this is not \"good\" code, just showing how it could be done\n",
    "\n",
    "\n",
    "X_numeric_test = data_test.loc[:, FEATURES].select_dtypes('number')\n",
    "\n",
    "X_text_values_test = embed(data_test['commentBody'].to_list())\n",
    "X_text_test = pd.DataFrame(data=X_text_values_test, columns=vectorizer.vocabulary_, index=data_test.index)\n",
    "\n",
    "\n",
    "X_test = pd.concat([X_numeric_test, X_text_test], axis=1)\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(X_train.columns == X_test.columns) # Quick Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part 2: Building and training an ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GridSearchCV(CatBoostClassifier(n_estimators=500, metric_period=100), param_grid={}, cv=3)\n",
    "\n",
    "model.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Estimating model performance and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True) # Check accuracy of a random guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This trained only one model, with estimated accuracy of 67% (as seen in column: 'mean_test_score')\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is good in comparison to a random guess, which would only have been 50% accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values = model.predict(X_test)\n",
    "predictions = pd.Series(prediction_values, X_test.index)\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
